{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "capable-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "swiss-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/etashguha/Downloads/final_mle_dataset.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ethical-acrobat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unlimited-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['product_name', 'product_description', 'prospect_name', 'prospect_industry', 'prospect_title', 'email', 'accepted', 'critique', 'edited', 'email_embedding', 'edited_embedding'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "arranged-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted = [obj for obj in data if obj.get('accepted') == 1]\n",
    "rejected = [obj for obj in data if obj.get('accepted') == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "danish-tuition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "functioning-saturday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "engaging-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_json_by_keys(json_data, keys):\n",
    "    separated_jsons = {}\n",
    "    for obj in json_data:\n",
    "        key_values = tuple(obj[key] for key in keys)\n",
    "        if key_values not in separated_jsons:\n",
    "            separated_jsons[key_values] = []\n",
    "        separated_jsons[key_values].append(obj)\n",
    "    return separated_jsons\n",
    "sort_key_list = ['prospect_industry']\n",
    "\n",
    "sorted_jsons = separate_json_by_keys(data, sort_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "decent-governor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/etashguha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/etashguha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_fillers(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Get the list of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize and filter out stopwords from the tokens\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token.lower(), pos='v') for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "opponent-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Capitalize the text\n",
    "    text = text.upper()\n",
    "\n",
    "    return text\n",
    "critical_words = []\n",
    "changed_words = []\n",
    "removed_words = []\n",
    "for reject in rejected:\n",
    "    critical_words.extend(preprocess_text(remove_fillers(reject[\"critique\"])).split(\" \"))\n",
    "    words_in_original_email = set(preprocess_text(remove_fillers(reject[\"email\"])).split(\" \"))\n",
    "    words_in_new_email = set(preprocess_text(remove_fillers(reject[\"edited\"])).split(\" \"))\n",
    "    changed_words.extend(list(words_in_new_email.difference(words_in_original_email)))\n",
    "    removed_words.extend(list(words_in_original_email.difference(words_in_new_email)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "strategic-density",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The email is too generic and lacks personalization. It doesn't establish a connection with the prospect or offer any value proposition. The use of 'unbeatable prices' may diminish the perceived value of the product.\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_most_common(lst, n):\n",
    "    counter = Counter(lst)\n",
    "    most_common = counter.most_common(n)\n",
    "    return most_common\n",
    "# find_most_common(critical_words, 100) \n",
    "rejected[300][\"critique\"]\n",
    "# find_most_common(changed_words, 100) \n",
    "# find_most_common(removed_words, 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ruled-station",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('entertainment', 19), ('fitness', 13), ('travel', 11), ('game', 10), ('education', 8), ('real', 7), ('estate', 7), ('beauty', 6), ('energy', 6), ('environmental', 4), ('arts', 4), ('transportation', 3), ('interior', 3), ('design', 3), ('architecture', 2), ('clean', 2), ('electrical', 2), ('art', 2), ('ecommerce', 2), ('agriculture', 2), ('law', 2), ('finance', 2), ('environment', 1), ('parent', 1), ('animal', 1), ('welfare', 1), ('', 1), ('dentistry', 1), ('care', 1), ('childcare', 1), ('tourism', 1), ('music', 1), ('landscape', 1), ('enforcement', 1), ('veterinary', 1), ('automotive', 1), ('renewable', 1), ('health', 1)]\n",
      "[('healthcare', 19), ('hospitality', 12), ('sport', 9), ('beverage', 8), ('food', 8), ('business', 5), ('fashion', 5), ('service', 4), ('none', 4), ('technology', 4), ('pediatrics', 3), ('engineer', 2), ('event', 2), ('plan', 2), ('market', 2), ('security', 2), ('construction', 2), ('photography', 1), ('publish', 1), ('manufacture', 1), ('advertise', 1), ('appliances', 1), ('science', 1), ('kitchen', 1), ('bath', 1), ('furniture', 1), ('translation', 1), ('home', 1), ('wellness', 1)]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "accepted_names = []\n",
    "\n",
    "def find_best_and_worst_categories(key):\n",
    "    for accept in accepted:\n",
    "        accepted_names.extend(remove_fillers(preprocess_text(accept[key])).split(\" \"))\n",
    "    rejected_names = []\n",
    "    for reject in rejected:\n",
    "        rejected_names.extend(remove_fillers(preprocess_text(reject[key])).split(\" \"))\n",
    "    good_names = copy.deepcopy(accepted_names)\n",
    "    for bad_name in rejected_names:\n",
    "        if bad_name in good_names:\n",
    "            good_names.remove(bad_name)\n",
    "    bad_names = copy.deepcopy(rejected_names)\n",
    "    for good_name in accepted_names:\n",
    "        if good_name in bad_names:\n",
    "            bad_names.remove(good_name)\n",
    "    print(find_most_common(good_names, 100))\n",
    "    print(find_most_common(bad_names, 100))\n",
    "\n",
    "find_best_and_worst_categories(\"prospect_industry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-christmas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
